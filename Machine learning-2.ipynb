{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677343ad-5d9a-4615-a8c0-256e67c860eb",
   "metadata": {},
   "source": [
    "# Machine Learning-2 assingnment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8677899-5182-49bb-9f2a-07a7752461f9",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c99a0c-fc64-4c13-9bcf-1e39cd6c2211",
   "metadata": {},
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe55b34-0f12-4dc6-9985-e75b9bb91f12",
   "metadata": {},
   "source": [
    "Overfitting - Overfittning occurs when a model performs good on training data  but falis to perform good when it sees new data.\n",
    "\n",
    "Consequences - The model may perform poorly on unseen or new data and it can lead to poor performence in real-world scenarios.\n",
    "\n",
    "Mitigated -1) We can increasing the size of the dataset so that i can help the model generalize better than before.\n",
    "\n",
    "2)- Feature Selection - Select only relevent feature to reduce  the model's complexity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824204c-4799-45ad-9a0d-e8a2385c234f",
   "metadata": {},
   "source": [
    "# Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c99d2-2401-4a6a-8f54-5e581127b94c",
   "metadata": {},
   "source": [
    "Underfitting - Underfitting occurs when a model performs poor on training data as well as on test data.\n",
    "\n",
    "Consequences - The model lacks the complexity to learn from the data, resulting in poor performance and an inability to generalize.\n",
    "\n",
    "Mitigated - 1) Use a different model: Experiment with different algorithms to find one that is better suited to the data.\n",
    "\n",
    "2) Increase model complexity: Use a more complex model, such as adding more layers to a neural network or increasing the polynomial degree in a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefdc5b6-3d02-42fa-9be2-e908d5950960",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1991bf-6abc-48e3-8d5a-6bcabbec0818",
   "metadata": {},
   "source": [
    "We can reduce overfitting with several methods are given below :-\n",
    "\n",
    "1) Increase the amount of training data\n",
    "\n",
    "providing more diverse and representative data can help the model to generalize better and reduce overfitting. A larger dataset allow the model to learn more and pattern.\n",
    "\n",
    "2) Feature Selection\n",
    "\n",
    "Choose only relevent feature and discard irrlevent feature.This will reduce the  model's complexity and help to focus on the important \n",
    "information from the data.\n",
    "\n",
    "3) Use simpler models:\n",
    "\n",
    "Choose simpler model architectures to avoid overfitting. For example, use a linear model instead of a complex non-linear model when the data does not exhibit complex patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6041195-b1ed-4ca7-b25c-c2a7aa97c605",
   "metadata": {},
   "source": [
    " Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4ab0a-00cf-494b-99bd-31aea4704126",
   "metadata": {},
   "source": [
    "Underfitting - Underfitting occurs when a model performs poor on training data as well as on test data.\n",
    "\n",
    "Scenarios where underfitting can occurs in ML.\n",
    "\n",
    "1) Insufficient Model Complexity:\n",
    "\n",
    "When the chosen model is too simple to capture the true relationships in the data. For example, using a linear regression model for a dataset with non-linear patterns.\n",
    "\n",
    "2) Small Training Dataset:\n",
    "\n",
    "With a small dataset, the model may not have enough examples to learn the underlying patterns effectively. Increasing the size of the training dataset can help address this issue.\n",
    "\n",
    "3) Limited Features:\n",
    "\n",
    "If the dataset is missing crucial features or if feature engineering is not performed adequately, the model may not have enough information to capture the underlying patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2e0ef-5baa-4a0e-8368-4dfd0a8e7229",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220804ad-fef5-4c18-b682-80f46be629ff",
   "metadata": {},
   "source": [
    "Bias:\n",
    "\n",
    "Bias refers to the error introduced by approximating a real-world problem with a simplified model. A high bias model is one that makes strong assumptions about the underlying data distribution and may not capture its complexity.\n",
    "\n",
    "Variance:\n",
    "\n",
    "Variance refers to the model's sensitivity to small fluctuations or noise in the training data. A high-variance model is one that is too flexible and captures the noise in the training data rather than the underlying patterns.\n",
    "\n",
    "Relationship between bais and variance:\n",
    "\n",
    "1) Low bias and High variance:\n",
    "\n",
    "A model with low bias and High variance fits the training data to well but when it comes to test data in not perform that well.\n",
    "\n",
    "2) High bias and Low variance:\n",
    "\n",
    "A model with high bias and low variance is simplistic and may not capture the true patterns in the data , and leading to poor performance on both training and test.\n",
    "\n",
    "3) Low bias and Low variance:\n",
    "\n",
    "A model with low bias and low variance is a generalizedn model that performs well on training and genelized well on test data.\n",
    "\n",
    "Effect on Model Performance:\n",
    "\n",
    "Underfitting (High Bias):\n",
    "\n",
    "When a model has high bias, it tends to underfit the data. It fails to capture the underlying patterns, leading to poor performance on both the training and test sets.\n",
    "\n",
    "Overfitting (High Variance):\n",
    "\n",
    "When a model has high variance, it tends to overfit the training data by capturing noise. While it performs well on the training set, it generalizes poorly to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d6d164-6a55-4c82-89a9-49be8e761b14",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be47987-4d70-4042-969f-d5f4693c627d",
   "metadata": {},
   "source": [
    "Training and Validation Curves:\n",
    "\n",
    "Plot the model's performance (e.g., accuracy, loss) on the training set and a validation set over multiple epochs. If the training performance improves while the validation performance plateaus or degrades, it may indicate overfitting. On the other hand, if both training and validation performance are poor, it may suggest underfitting.\n",
    "\n",
    "Learning Curves:\n",
    "\n",
    "Learning curves display the training and validation performance as a function of the training set size. Analyzing learning curves can help identify overfitting or underfitting based on trends in performance with varying amounts of data.\n",
    "\n",
    "Holdout Validation Set:\n",
    "\n",
    "Split the dataset into training and validation sets. Train the model on the training set and evaluate its performance on the validation set. A significant gap between training and validation performance may indicate overfitting.\n",
    "\n",
    "Cross-Validation:\n",
    "\n",
    "Use k-fold cross-validation to assess the model's performance across different data splits. If the model performs well on some folds but poorly on others, it might be an indication of overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d744a-500c-4e8e-9348-b0a738dc7d69",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c77aa6-0c4c-4907-9a82-b38f54798946",
   "metadata": {},
   "source": [
    "Bias:\n",
    "\n",
    "Definition: Bias is the error introduced by approximating a real-world problem with a simplified model. It represents the model's tendency to make systematic errors by consistently deviating from the true values.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    ".  High bias models are too simplistic and may not capture the underlying patterns in the data.\n",
    "\n",
    ".  These models make strong assumptions about the data distribution.\n",
    "\n",
    ".  They result in underfitting, performing poorly on both training and test sets.\n",
    "\n",
    "Example:\n",
    "A linear regression model applied to a dataset with complex non-linear relationships.\n",
    "\n",
    "Variance:\n",
    "\n",
    "Definition: Variance is the error introduced by the model's sensitivity to small fluctuations or noise in the training data. It represents the model's tendency to capture the noise rather than the underlying patterns.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    ".  High variance models are too flexible and may fit the training data too closely, capturing noise.\n",
    "\n",
    ".  These models are sensitive to changes in the training set and may not generalize well to new data.\n",
    "\n",
    ".  They result in overfitting, performing well on the training set but poorly on the test set.\n",
    "\n",
    "Example:\n",
    "\n",
    "A high-degree polynomial regression model applied to a dataset with limited training examples.\n",
    "\n",
    "Comparison:\n",
    "\n",
    "Bias and Variance Tradeoff:\n",
    "\n",
    "There is a tradeoff between bias and variance. Increasing model complexity often decreases bias but increases variance, and vice versa.\n",
    "\n",
    "Impact on Model Performance:\n",
    "\n",
    "High bias models result in underfitting and poor performance on both training and test sets.\n",
    "\n",
    "High variance models result in overfitting, performing well on the training set but poorly on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622ca1f-f140-4a66-9a96-ef0ef8f27a2c",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ecd1a-8724-450b-a4e8-587851ba3dec",
   "metadata": {},
   "source": [
    "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the model's objective function. The goal is to discourage overly complex models that may fit the training data too closely, capturing noise and leading to poor generalization. \n",
    "\n",
    "Common regularization techniques:\n",
    "\n",
    "1) L1 Regularization (Lasso):\n",
    "\n",
    "Objective Function Modification:\n",
    "\n",
    "Original Objective = Loss + λ * Σ|wi|\n",
    "How it Works:\n",
    "\n",
    ". Adds the absolute values of the weights (coefficients) as a penalty to the loss function.\n",
    "\n",
    ". Encourages sparsity in the weight values, effectively selecting a subset of important features.\n",
    "\n",
    "Use Case:\n",
    "\n",
    "When there is a belief that many features are irrelevant or redundant.\n",
    "\n",
    "2) L2 Regularization (Ridge):\n",
    "\n",
    "Objective Function Modification:\n",
    "Original Objective = Loss + λ * Σwi²\n",
    "\n",
    "How it Works:\n",
    "\n",
    ". Adds the squared values of the weights as a penalty to the loss function.\n",
    "\n",
    ". Encourages the weights to be small, preventing any single feature from dominating the model.\n",
    "\n",
    "Use Case:\n",
    "\n",
    ". When all features are expected to be relevant but with modest contributions.\n",
    "\n",
    "3) Elastic Net Regularization:\n",
    "\n",
    "Objective Function Modification:\n",
    "Original Objective = Loss + λ₁ * Σ|wi| + λ₂ * Σwi²\n",
    "\n",
    "How it Works:\n",
    "\n",
    ". Combines both L1 and L2 penalties, providing a balance between feature selection and weight shrinkage.\n",
    "\n",
    ". Controlled by two hyperparameters (λ₁ and λ₂).\n",
    "\n",
    "Use Case:\n",
    "\n",
    ".  When a dataset has many features, some of which may be irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45702e-b41b-4699-bd99-94ee997932db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef5752b-2cbe-4ab1-9b24-c3101d79cfea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
